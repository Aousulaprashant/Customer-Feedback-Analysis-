# -*- coding: utf-8 -*-
"""Customer-Feedback.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XNal3A82d1u_W7td-MTcK7RL0TOiWzLA

#ABOUT THE PROJECT

# **Customer Feedback Analysis Using Hierarchical Clustering**  

## **ðŸ“Œ Project Overview**  
Customer feedback is a crucial aspect of business success. In this project, we analyze **Twitter airline reviews** using **Hierarchical Clustering** to identify patterns in customer sentiment. Instead of using predefined sentiment labels, clustering helps discover **hidden groupings** of similar feedback, providing insights into customer satisfaction and common issues.

## **ðŸŽ¯ Objectives**  
- Preprocess customer feedback text from Twitter airline reviews.  
- Convert textual data into numerical format using **TF-IDF vectorization**.  
- Apply **Hierarchical Clustering** to group similar customer reviews.  
- Visualize clusters using **dendrograms and word clouds**.  
- Analyze patterns in customer sentiment without relying on predefined labels.  

## **ðŸ“‚ Dataset Used**  
- **Dataset Name**: [Twitter US Airline Sentiment](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment)  
- **Key Features Selected**:  
  - **`text`** â†’ The actual customer review (Main input for clustering).  
  - **(Optional) `airline`** â†’ If we want airline-based clustering.  
  - **(Optional) `negativereason`** â†’ If we want sub-clustering for negative reviews.  

## **ðŸ”„ Methodology**  
### **1. Data Preprocessing**  
- Remove special characters, numbers, and stopwords.  
- Convert text to lowercase and tokenize words.  
- Apply **TF-IDF Vectorization** to transform text into numerical features.  

### **2. Clustering Approach**  
- Use **Hierarchical Clustering (Agglomerative Clustering)**.  
- Experiment with different **linkage methods** (ward, complete, average).  
- Determine the number of clusters using a **dendrogram**.  

### **3. Visualization**  
- Plot a **dendrogram** to observe hierarchical relationships between clusters.  
- Use **word clouds** to understand common words in each cluster.  

## **ðŸ” Expected Insights**  
- Identify common themes in customer complaints and praises.  
- Detect **which airlines** receive similar types of feedback.  
- Recognize patterns in **negative reviews** (e.g., delays, customer service issues).  

## **ðŸš€ Future Enhancements**  
- Compare clustering results with sentiment labels to evaluate effectiveness.  
- Apply **DBSCAN or K-Means** for alternative clustering approaches.  
- Implement **topic modeling** (LDA) to extract topics from reviews.  

---

#IMPORTING LIBRARIES AND LOADING CUSTOMER DATA
"""

import pandas as pd

#for preprocessing
import nltk
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA

#model building
import numpy as np
import scipy.cluster.hierarchy as sc
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering

customer_data=pd.read_csv('/content/Tweets.csv')

customer_data.head()

customer_tweet_text=customer_data['text']

customer_data['airline_sentiment'].value_counts()

customer_tweet_text.head()

customer_tweet_text = customer_tweet_text.replace('@VirginAmerica', '', regex=True)
customer_tweet_text.head()

customer_tweet_text.isnull().sum()

"""- Text Preprocessing:

  --> Tokenization (splitting sentences into words).

  --> Removing stopwords (common words like "and", "the", etc. that don't carry much meaning).

  --> Lemmatization/Stemming (reducing words to their base forms).
Convert text to lowercase.

  --> Remove unnecessary symbols, numbers, and punctuation.
- Text Vectorization

Convert text data into numerical format using techniques such as:
TF-IDF (Term Frequency-Inverse Document Frequency): This helps to weigh words based on their importance within the document and across the dataset.
"""

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

def TextPreprocessing(text):
  text=text.lower()
  text = re.sub(r'[^a-z\s]', '', text)
  tokens = word_tokenize(text)
  stop_words = set(stopwords.words('english'))  # Use NLTK stopwords
  tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords
  return " ".join(tokens)

customer_tweet_text = customer_tweet_text.apply(TextPreprocessing)

customer_tweet_text.head()

#vectorization ,converting into numarical format

vectorizer=TfidfVectorizer(max_features=500)
X_tfidf=vectorizer.fit_transform(customer_tweet_text).toarray()

print(vectorizer.get_feature_names_out())

# Apply PCA for dimensionality reduction
pca = PCA(n_components=50)  # Reduce to 50 dimensions
X_pca = pca.fit_transform(X_tfidf)

"""#CLUTERING

- creating dendogram
"""

# Convert sparse matrix to dense numpy array


linked = sc.linkage(X_pca[:100], method='ward')

# Plot the dendrogram
plt.figure(figsize=(10, 5))
sc.dendrogram(linked, truncate_mode='level', p=10, leaf_rotation=90, leaf_font_size=10)
plt.title("Dendrogram for Twitter Sentiment")
plt.xlabel("Sample Index")
plt.ylabel("Distance")
plt.show()

from sklearn.metrics import silhouette_score
from sklearn.cluster import AgglomerativeClustering

silhouette_scores = {}
for k in range(2, 10):  # Try different cluster numbers
    clustering = AgglomerativeClustering(n_clusters=k, linkage='ward')
    labels = clustering.fit_predict(X_pca)
    score = silhouette_score(X_pca, labels)
    silhouette_scores[k] = score
    print(f"n_clusters={k}, Silhouette Score={score:.4f}")

# Find the best n_clusters
best_k = max(silhouette_scores, key=silhouette_scores.get)
print(f"Optimal number of clusters: {best_k}")

# Hierarchical Clustering
agglo_clustering = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')
clusters = agglo_clustering.fit_predict(X_pca)

customer_data['cluster'] = clusters

customer_data.head()

customer_data['text']

customer_data['cluster'].value_counts()

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Reduce dimensions to 2D using PCA
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X_pca)  # X_dense is the dense representation of the TF-IDF matrix

# Scatter plot of the clusters
plt.figure(figsize=(12, 8))
plt.scatter(
    X_reduced[:, 0], X_reduced[:, 1], c=agglo_clustering.labels_, cmap='viridis', alpha=0.7
)
plt.title("Clusters of Tweets (PCA Visualization)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()

# Hierarchical Clustering
agglo_clustering = AgglomerativeClustering(n_clusters=3, affinity='cosine', linkage='ward')
clusters = agglo_clustering.fit_predict(X_pca)

customer_data['cluster'] = clusters

# Reduce dimensions to 2D using PCA
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X_pca)  # X_dense is the dense representation of the TF-IDF matrix

# Scatter plot of the clusters
plt.figure(figsize=(12, 8))
plt.scatter(
    X_reduced[:, 0], X_reduced[:, 1], c=agglo_clustering.labels_, cmap='viridis', alpha=0.7
)
plt.title("Clusters of Tweets (PCA Visualization)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()